import sys
import os
import json
import time
import re
import random
import tempfile
import uuid
from datetime import datetime, timedelta
from urllib.parse import quote, urljoin

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, WebDriverException
    from selenium.webdriver.common.action_chains import ActionChains
    
    class CombinedSolarCrawler:
        def __init__(self):
            self.gov_search_url = "https://sousuo.www.gov.cn/sousuo/search.shtml"
            self.nea_search_url = "https://www.nea.gov.cn/search.htm"
            self.driver = None
            
        def setup_driver(self):
            """è®¾ç½®æµè§ˆå™¨ç¯å¢ƒ"""
            chrome_options = Options()

            # ä½¿ç”¨æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--headless=new')
            chrome_options.add_argument('--disable-gpu')

            # åæ£€æµ‹é…ç½®
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            # ä¸ä½¿ç”¨user-data-dirï¼Œè®©Chromeè‡ªåŠ¨å¤„ç†ï¼ˆé¿å…å†²çªï¼‰
            # unique_dir = os.path.join(tempfile.gettempdir(), f"chrome_combined_{uuid.uuid4().hex}")
            # chrome_options.add_argument(f'--user-data-dir={unique_dir}')

            # æ·»åŠ æ›´å¤šéš”ç¦»å‚æ•°
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument(f'--remote-debugging-port={9222 + random.randint(0, 1000)}')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            chrome_options.add_argument('--window-size=1920,1080')
            
            try:
                self.driver = webdriver.Chrome(options=chrome_options)
                self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
                return True
            except Exception as e:
                print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
                return False
        
        def close_driver(self):
            """å…³é—­æµè§ˆå™¨"""
            if self.driver:
                self.driver.quit()
                self.driver = None

        def crawl_all_sources(self, pages=5):
            """çˆ¬å–æ‰€æœ‰æ•°æ®æº"""
            all_news = []
            
            # 1. ä¸­å›½æ”¿åºœç½‘ - å¤šä¸ªå…³é”®è¯
            gov_keywords = ['å…‰ä¼', 'å¤ªé˜³èƒ½', 'æ–°èƒ½æº', 'èƒ½æº', 'ç”µåŠ›', 'ç¯ä¿', 'å¯æŒç»­å‘å±•']
            for keyword in gov_keywords:
                print(f"\nğŸ” å¼€å§‹çˆ¬å–ä¸­å›½æ”¿åºœç½‘å…³é”®è¯: '{keyword}'")
                gov_news = self.crawl_gov_news(keyword, pages=3)
                all_news.extend(gov_news)
                time.sleep(2)
            
            # 2. å›½å®¶èƒ½æºå±€ - å¤šä¸ªå…³é”®è¯
            print(f"\nğŸ” å¼€å§‹çˆ¬å–å›½å®¶èƒ½æºå±€")
            nea_news = self.crawl_nea_news(pages=pages)
            all_news.extend(nea_news)
            
            return all_news

        def crawl_gov_news(self, keyword="å…‰ä¼", pages=5):
            """çˆ¬å–ä¸­å›½æ”¿åºœç½‘æ–°é—»"""
            if not self.driver and not self.setup_driver():
                return []
            
            all_news = []
            
            try:
                # æ„å»ºåˆå§‹æœç´¢URL
                initial_url = f"{self.gov_search_url}?code=17da70961a7&dataTypeId=107&searchWord={quote(keyword)}"
                print(f"   è®¿é—®åˆå§‹URL: {initial_url}")
                
                self.driver.get(initial_url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                WebDriverWait(self.driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                time.sleep(5)
                
                # æå–ç¬¬ä¸€é¡µæ•°æ®
                page_news = self.extract_gov_news(keyword)
                all_news.extend(page_news)
                print(f"   âœ… ç¬¬ 1 é¡µæ‰¾åˆ° {len(page_news)} æ¡æ–°é—»")
                
                # æ˜¾ç¤ºå‰3æ¡æ–°é—»æ ‡é¢˜
                for i, news in enumerate(page_news[:3]):
                    print(f"      {i+1}. {news['title'][:50]}...")
                
                # æ¨¡æ‹Ÿç‚¹å‡»ç¿»é¡µè·å–åç»­é¡µé¢
                for page in range(2, pages + 1):
                    print(f"ğŸ” æ­£åœ¨è·å–ç¬¬ {page} é¡µ...")
                    
                    if self.click_gov_next_page():
                        time.sleep(4)
                        
                        page_news = self.extract_gov_news(keyword)
                        all_news.extend(page_news)
                        
                        print(f"   âœ… ç¬¬ {page} é¡µæ‰¾åˆ° {len(page_news)} æ¡æ–°é—»")
                        
                        # æ˜¾ç¤ºå‰3æ¡æ–°é—»æ ‡é¢˜
                        for i, news in enumerate(page_news[:3]):
                            print(f"      {i+1}. {news['title'][:50]}...")
                        
                        if page < pages:
                            delay = random.uniform(2, 4)
                            time.sleep(delay)
                    else:
                        print(f"   âŒ ç¬¬ {page} é¡µç¿»é¡µå¤±è´¥ï¼Œåœæ­¢çˆ¬å–")
                        break
                
                return all_news
                
            except Exception as e:
                print(f"âŒ ä¸­å›½æ”¿åºœç½‘çˆ¬å–å¤±è´¥: {e}")
                return []
        
        def click_gov_next_page(self):
            """æ¨¡æ‹Ÿç‚¹å‡»ä¸­å›½æ”¿åºœç½‘ä¸‹ä¸€é¡µæŒ‰é’®"""
            try:
                # å¤šç§ä¸‹ä¸€é¡µæŒ‰é’®é€‰æ‹©å™¨
                next_selectors = [
                    "a.next",
                    ".next-page", 
                    "li.next > a",
                    "div.page > a:last-child",
                    "//a[contains(text(), 'ä¸‹ä¸€é¡µ')]",
                    "//a[contains(text(), '>')]",
                    "//a[contains(@class, 'next')]"
                ]
                
                for selector in next_selectors:
                    try:
                        if selector.startswith("//"):
                            next_buttons = self.driver.find_elements(By.XPATH, selector)
                        else:
                            next_buttons = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        
                        for button in next_buttons:
                            if button.is_displayed() and button.is_enabled():
                                print(f"   æ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®: {button.text}")
                                
                                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", button)
                                time.sleep(1)
                                
                                actions = ActionChains(self.driver)
                                actions.move_to_element(button).click().perform()
                                
                                print("   âœ… æˆåŠŸç‚¹å‡»ä¸‹ä¸€é¡µ")
                                return True
                    except:
                        continue
                
                # å°è¯•æŸ¥æ‰¾é¡µç é“¾æ¥
                print("   å°è¯•æŸ¥æ‰¾é¡µç é“¾æ¥...")
                page_links = self.driver.find_elements(By.CSS_SELECTOR, "a.page, .page a, li a, .pagination a")
                current_page = self.get_current_page()
                
                for link in page_links:
                    try:
                        link_text = link.text.strip()
                        if link_text.isdigit() and int(link_text) == current_page + 1:
                            if link.is_displayed() and link.is_enabled():
                                print(f"   æ‰¾åˆ°é¡µç é“¾æ¥: {link_text}")
                                
                                self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", link)
                                time.sleep(1)
                                
                                actions = ActionChains(self.driver)
                                actions.move_to_element(link).click().perform()
                                
                                print("   âœ… æˆåŠŸç‚¹å‡»é¡µç é“¾æ¥")
                                return True
                    except:
                        continue
                
                print("   âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä¸‹ä¸€é¡µæŒ‰é’®")
                return False
                
            except Exception as e:
                print(f"   âŒ ç¿»é¡µå¤±è´¥: {e}")
                return False

        def crawl_nea_news(self, pages=5):
            """çˆ¬å–å›½å®¶èƒ½æºå±€æ–°é—»"""
            if not self.driver and not self.setup_driver():
                return []
            
            all_news = []
            
            try:
                # å›½å®¶èƒ½æºå±€å¤šä¸ªå…³é”®è¯
                nea_keywords = ['å…‰ä¼', 'å¤ªé˜³èƒ½', 'æ–°èƒ½æº', 'èƒ½æº', 'ç”µåŠ›', 'ç¯ä¿', 'ç¢³è¾¾å³°', 'ç¢³ä¸­å’Œ']
                
                for keyword in nea_keywords:
                    print(f"\nğŸ” å¼€å§‹æœç´¢å›½å®¶èƒ½æºå±€å…³é”®è¯: '{keyword}'")
                    
                    encoded_keyword = quote(keyword, encoding='utf-8')
                    search_url = f"{self.nea_search_url}?kw={encoded_keyword}"
                    
                    print(f"   è®¿é—®URL: {search_url}")
                    
                    self.driver.get(search_url)
                    
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    
                    time.sleep(6)
                    
                    # æå–å½“å‰å…³é”®è¯çš„æ–°é—»
                    keyword_news = self.extract_nea_news(keyword)
                    all_news.extend(keyword_news)
                    
                    print(f"   âœ… å…³é”®è¯ '{keyword}' æ‰¾åˆ° {len(keyword_news)} æ¡æ–°é—»")
                    
                    # æ˜¾ç¤ºå‰3æ¡æ–°é—»æ ‡é¢˜
                    for i, news in enumerate(keyword_news[:3]):
                        print(f"      {i+1}. {news['title'][:50]}...")
                    
                    # å…³é”®è¯é—´å»¶è¿Ÿ
                    if keyword != nea_keywords[-1]:
                        time.sleep(3)
                
                return all_news
                
            except Exception as e:
                print(f"âŒ å›½å®¶èƒ½æºå±€çˆ¬å–å¤±è´¥: {e}")
                return []
        
        def extract_nea_news(self, keyword):
            """æå–å›½å®¶èƒ½æºå±€æ–°é—» - ä¸è¿‡æ»¤"""
            news_list = []
            
            try:
                # è·å–æ‰€æœ‰é“¾æ¥
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                print(f"   é¡µé¢å…±æœ‰ {len(all_links)} ä¸ªé“¾æ¥")
                
                for link in all_links:
                    try:
                        title = link.text.strip()
                        href = link.get_attribute('href')
                        
                        # å®½æ¾çš„è¿‡æ»¤æ¡ä»¶ - åªæ’é™¤æ˜æ˜¾æ— å…³çš„å¯¼èˆªé“¾æ¥
                        if (title and len(title) > 8 and 
                            href and ('nea.gov.cn' in href or href.startswith('/')) and
                            not any(word in title for word in ['é¦–é¡µ', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ', 'ç½‘ç«™', 'å¯¼èˆª', 'æ›´å¤š', 'è¿”å›'])):
                            
                            # æ„å»ºå®Œæ•´URL
                            if href.startswith('/'):
                                full_url = f"https://www.nea.gov.cn{href}"
                            else:
                                full_url = href
                            
                            # æå–æ—¥æœŸ
                            date = self._extract_date_near_element(link)
                            
                            news_data = {
                                'title': title,
                                'link': full_url,
                                'date': date,
                                'source': 'å›½å®¶èƒ½æºå±€',
                                'keyword': keyword
                            }
                            
                            # å»é‡æ£€æŸ¥
                            if not any(n['title'] == title for n in news_list):
                                news_list.append(news_data)
                                
                    except:
                        continue
                
                print(f"   æ‰¾åˆ° {len(news_list)} æ¡å›½å®¶èƒ½æºå±€æ–°é—»")
                return news_list
                        
            except Exception as e:
                print(f"   æå–å›½å®¶èƒ½æºå±€æ–°é—»å¤±è´¥: {e}")
                return []
        
        def extract_gov_news(self, keyword):
            """æå–ä¸­å›½æ”¿åºœç½‘æ–°é—»å†…å®¹ - ä¸è¿‡æ»¤"""
            news_list = []
            
            try:
                all_links = self.driver.find_elements(By.TAG_NAME, "a")
                print(f"   é¡µé¢å…±æœ‰ {len(all_links)} ä¸ªé“¾æ¥")
                
                for link in all_links:
                    try:
                        title = link.text.strip()
                        href = link.get_attribute('href')
                        
                        # å®½æ¾çš„è¿‡æ»¤æ¡ä»¶ - åªæ’é™¤æ˜æ˜¾æ— å…³çš„å¯¼èˆªé“¾æ¥
                        if (title and len(title) > 8 and 
                            href and 'gov.cn' in href and
                            not any(word in title for word in ['é¦–é¡µ', 'ä¸Šä¸€é¡µ', 'ä¸‹ä¸€é¡µ', 'ç½‘ç«™', 'å¯¼èˆª', 'æ›´å¤š', '>>', '>', 'è¿”å›'])):
                            
                            date = self._extract_date_near_element(link)
                            
                            news_data = {
                                'title': title,
                                'link': href,
                                'date': date,
                                'source': 'ä¸­å›½æ”¿åºœç½‘',
                                'keyword': keyword
                            }
                            
                            if not any(n['title'] == title for n in news_list):
                                news_list.append(news_data)
                                    
                    except:
                        continue
                
                print(f"   æ‰¾åˆ° {len(news_list)} æ¡ä¸­å›½æ”¿åºœç½‘æ–°é—»")
                return news_list
                        
            except Exception as e:
                print(f"   æå–æ–°é—»å¤±è´¥: {e}")
                return []
        
        def get_current_page(self):
            """è·å–å½“å‰é¡µç """
            try:
                active_selectors = [
                    ".page .active", ".current", "li.active a", "span.current"
                ]
                
                for selector in active_selectors:
                    try:
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        for elem in elements:
                            text = elem.text.strip()
                            if text.isdigit():
                                return int(text)
                    except:
                        continue
                
                return 1
            except:
                return 1
        
        def _extract_date_near_element(self, element):
            """åœ¨å…ƒç´ é™„è¿‘æå–æ—¥æœŸ"""
            try:
                parent = element.find_element(By.XPATH, "./..")
                parent_text = parent.text
                
                date_patterns = [
                    r'\d{4}-\d{2}-\d{2}',
                    r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
                    r'\d{4}/\d{1,2}/\d{1,2}',
                    r'å…¥åº“æ—¶é—´ï¼š(\d{4}-\d{2}-\d{2})',
                    r'å‘å¸ƒæ—¶é—´ï¼š(\d{4}-\d{2}-\d{2})'
                ]
                
                for pattern in date_patterns:
                    match = re.search(pattern, parent_text)
                    if match:
                        date_str = match.group(1) if len(match.groups()) > 0 else match.group()
                        date_str = re.sub(r'[å¹´æœˆ/]', '-', date_str.replace('æ—¥', ''))
                        return date_str
                        
                siblings = parent.find_elements(By.XPATH, "./*")
                for sibling in siblings:
                    try:
                        sibling_text = sibling.text
                        for pattern in date_patterns:
                            match = re.search(pattern, sibling_text)
                            if match:
                                date_str = match.group(1) if len(match.groups()) > 0 else match.group()
                                date_str = re.sub(r'[å¹´æœˆ/]', '-', date_str.replace('æ—¥', ''))
                                return date_str
                    except:
                        continue
            except:
                pass
            
            return datetime.now().strftime('%Y-%m-%d')
        
        def process_and_save_news(self, news_list, filename='combined_news.json'):
            """å¤„ç†å¹¶ä¿å­˜æ–°é—»æ•°æ®"""
            seen_titles = set()
            unique_news = []
            
            for news in news_list:
                clean_title = re.sub(r'\s+', ' ', news['title']).strip()
                if clean_title and clean_title not in seen_titles:
                    seen_titles.add(clean_title)
                    unique_news.append(news)
            
            # æŒ‰æ—¥æœŸæ’åº
            unique_news.sort(key=lambda x: x['date'], reverse=True)
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(unique_news, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {filename}ï¼Œå…± {len(unique_news)} æ¡æ–°é—»")
                return True
            except Exception as e:
                print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
                return False
        
        def get_news_data(self, pages=5, save_file='combined_news.json'):
            """ä¸»æ–¹æ³•ï¼šè·å–æ‰€æœ‰æ–°é—»æ•°æ®"""
            print(f"ğŸš€ å¼€å§‹çˆ¬å–æ‰€æœ‰æ•°æ®æºçš„æ–°é—»...")
            
            news_data = self.crawl_all_sources(pages)
            
            if news_data:
                print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®æºçˆ¬å–å®Œæˆï¼")
                print(f"ğŸ“Š åŸå§‹æ•°æ®æ€»è®¡ï¼š{len(news_data)} æ¡")
                
                # æŒ‰æ¥æºç»Ÿè®¡
                gov_count = len([n for n in news_data if n['source'] == 'ä¸­å›½æ”¿åºœç½‘'])
                nea_count = len([n for n in news_data if n['source'] == 'å›½å®¶èƒ½æºå±€'])
                print(f"   ä¸­å›½æ”¿åºœç½‘: {gov_count} æ¡")
                print(f"   å›½å®¶èƒ½æºå±€: {nea_count} æ¡")
                
                # å¤„ç†å¹¶ä¿å­˜æ•°æ®
                self.process_and_save_news(news_data, save_file)
                
                print(f"\nğŸ“° å‰20æ¡æœ€æ–°æ–°é—»:")
                for i, news in enumerate(news_data[:20], 1):
                    print(f"{i:2d}. [{news['source']}][{news['date']}] {news['title'][:60]}...")
                
                return news_data
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–°é—»æ•°æ®")
                return []
    
    # ç‹¬ç«‹è¿è¡Œæµ‹è¯•
    def main():
        crawler = CombinedSolarCrawler()
        
        try:
            # è·å–æ‰€æœ‰æ–°é—»æ•°æ®
            news_data = crawler.get_news_data(
                pages=5, 
                save_file='combined_news.json'
            )
            
            return news_data
            
        finally:
            crawler.close_driver()
    
    if __name__ == "__main__":
        main()
        
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·å®‰è£…Selenium: pip install selenium")