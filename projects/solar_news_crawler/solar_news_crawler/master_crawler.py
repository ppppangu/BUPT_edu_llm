# master_crawler.py
import os
import json
import time
import schedule
import shutil
import glob
import subprocess
import sys
from datetime import datetime

def save_individual_crawler_data(crawler_name, data, output_dir="output/individual"):
    """ä¿å­˜å•ä¸ªçˆ¬è™«çš„æ•°æ®åˆ°ç‹¬ç«‹æ–‡ä»¶"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"{crawler_name}_{timestamp}.json"
    filepath = os.path.join(output_dir, filename)

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"{crawler_name}æ•°æ®å·²ä¿å­˜: {filepath}")
    return filepath

def cleanup_chrome_temp():
    """æ¸…ç†Chromeä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å¤šå®ä¾‹å†²çª"""
    try:
        import tempfile
        temp_dir = tempfile.gettempdir()

        # æ¸…ç†Chromeç›¸å…³çš„ä¸´æ—¶ç›®å½•
        patterns = [
            os.path.join(temp_dir, 'chrome_*'),
            os.path.join(temp_dir, '.com.google.Chrome.*'),
            os.path.join(temp_dir, 'scoped_dir*')
        ]

        for pattern in patterns:
            for path in glob.glob(pattern):
                try:
                    if os.path.isdir(path):
                        shutil.rmtree(path, ignore_errors=True)
                except:
                    pass

        print("ğŸ§¹ å·²æ¸…ç†Chromeä¸´æ—¶æ–‡ä»¶")
    except Exception as e:
        print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")


def run_single_crawler_subprocess(crawler_name, timeout=600):
    """
    åœ¨ç‹¬ç«‹å­è¿›ç¨‹ä¸­è¿è¡Œå•ä¸ªçˆ¬è™«

    Args:
        crawler_name: çˆ¬è™«åç§° (iea, pvmagazine, irena, combined)
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10åˆ†é’Ÿ

    Returns:
        dict: {'success': bool, 'file': str, 'count': int, 'error': str}
    """
    try:
        print(f"\n{'='*60}")
        print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸš€ å¯åŠ¨ {crawler_name.upper()} çˆ¬è™«ï¼ˆç‹¬ç«‹è¿›ç¨‹ï¼‰")
        print(f"â±ï¸  è¶…æ—¶è®¾ç½®: {timeout}ç§’")
        print(f"{'='*60}\n")

        # æ„å»ºçˆ¬è™«è„šæœ¬è·¯å¾„
        script_map = {
            'iea': 'iea_crawler.py',
            'pvmagazine': 'pv_magazine_crawler.py',
            'irena': 'irena_crawler.py',
            'combined': 'combined_crawler.py'
        }

        if crawler_name not in script_map:
            return {
                'success': False,
                'error': f'æœªçŸ¥çš„çˆ¬è™«åç§°: {crawler_name}',
                'file': None,
                'count': 0
            }

        script_path = os.path.join(os.path.dirname(__file__), script_map[crawler_name])

        # è¿è¡Œçˆ¬è™«è„šæœ¬ï¼ˆå¸¦è¶…æ—¶ï¼‰
        result = subprocess.run(
            [sys.executable, script_path],
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=timeout,
            cwd=os.path.dirname(__file__)
        )

        # è¾“å‡ºæ—¥å¿—
        if result.stdout:
            print(result.stdout)

        if result.stderr:
            print(f"âš ï¸  stderrè¾“å‡º:\n{result.stderr}")

        # æ£€æŸ¥æ‰§è¡Œç»“æœ
        if result.returncode == 0:
            # æŸ¥æ‰¾æœ€æ–°çš„è¾“å‡ºæ–‡ä»¶
            output_dir = "output/individual"
            pattern = os.path.join(output_dir, f"{crawler_name}_*.json")
            files = glob.glob(pattern)

            if files:
                # è·å–æœ€æ–°æ–‡ä»¶
                latest_file = max(files, key=os.path.getmtime)

                # è¯»å–æ•°æ®ç»Ÿè®¡
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        count = len(data) if isinstance(data, list) else 0
                except:
                    count = 0

                print(f"âœ… {crawler_name.upper()} çˆ¬è™«å®Œæˆï¼è·å– {count} æ¡æ•°æ®")
                print(f"ğŸ“ æ–‡ä»¶: {latest_file}\n")

                return {
                    'success': True,
                    'file': latest_file,
                    'count': count,
                    'error': None
                }
            else:
                print(f"âš ï¸  {crawler_name.upper()} çˆ¬è™«å®Œæˆï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶\n")
                return {
                    'success': False,
                    'error': 'æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶',
                    'file': None,
                    'count': 0
                }
        else:
            error_msg = f"çˆ¬è™«è„šæœ¬è¿”å›é”™è¯¯ç : {result.returncode}"
            print(f"âŒ {crawler_name.upper()} çˆ¬è™«å¤±è´¥: {error_msg}\n")
            return {
                'success': False,
                'error': error_msg,
                'file': None,
                'count': 0
            }

    except subprocess.TimeoutExpired:
        error_msg = f"è¶…æ—¶ï¼ˆ>{timeout}ç§’ï¼‰"
        print(f"â±ï¸  {crawler_name.upper()} çˆ¬è™«è¶…æ—¶: {error_msg}\n")
        return {
            'success': False,
            'error': error_msg,
            'file': None,
            'count': 0
        }
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ {crawler_name.upper()} çˆ¬è™«æ‰§è¡Œå‡ºé”™: {error_msg}\n")
        return {
            'success': False,
            'error': error_msg,
            'file': None,
            'count': 0
        }


def run_all_crawlers(timeout_per_crawler=600):
    """
    è¿è¡Œæ‰€æœ‰çˆ¬è™«ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰

    Args:
        timeout_per_crawler: æ¯ä¸ªçˆ¬è™«çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        dict: æ‰§è¡Œç»“æœç»Ÿè®¡
    """
    try:
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        print(f"â±ï¸  å•ä¸ªçˆ¬è™«è¶…æ—¶: {timeout_per_crawler}ç§’")
        print(f"{'='*80}\n")

        crawlers = ['iea', 'pvmagazine', 'irena', 'combined']
        results = {}

        # ä¸²è¡Œæ‰§è¡Œæ‰€æœ‰çˆ¬è™«
        for crawler_name in crawlers:
            results[crawler_name] = run_single_crawler_subprocess(
                crawler_name,
                timeout_per_crawler
            )

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å†²çª
            print("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...\n")
            cleanup_chrome_temp()
            time.sleep(5)  # çŸ­æš‚ç­‰å¾…

        # ç»Ÿè®¡ç»“æœ
        print(f"\n{'='*80}")
        print("ğŸ“Š çˆ¬è™«ä»»åŠ¡æ‰§è¡Œç»“æœæ±‡æ€»")
        print(f"{'='*80}\n")

        total_success = 0
        total_count = 0
        failed_crawlers = []

        for crawler_name, result in results.items():
            status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
            count = result['count']
            error = result.get('error', '')

            print(f"  {crawler_name.upper():12s} {status:8s} - {count:3d}æ¡æ•°æ®", end="")
            if error:
                print(f"  ({error})")
            else:
                print()

            if result['success']:
                total_success += 1
                total_count += count
            else:
                failed_crawlers.append(crawler_name)

        print(f"\n{'='*80}")
        print(f"âœ… æˆåŠŸ: {total_success}/{len(crawlers)} ä¸ªçˆ¬è™«")
        print(f"ğŸ“Š æ€»è®¡: {total_count} æ¡æ•°æ®")
        if failed_crawlers:
            print(f"âŒ å¤±è´¥: {', '.join(failed_crawlers)}")
        print(f"{'='*80}\n")

        # 5. è¿è¡Œç¿»è¯‘
        print(f"\n{'='*60}")
        print("ğŸŒ å¼€å§‹ç¿»è¯‘ä»»åŠ¡")
        print(f"{'='*60}\n")

        translator_output = None
        try:
            from translator import MultiFileTranslator
            translator = MultiFileTranslator()
            translator_output = translator.merge_and_save_translations()
            if translator_output:
                print(f"\nâœ… ç¿»è¯‘å®Œæˆï¼")
                print(f"ğŸ“ æ–‡ä»¶: {translator_output}\n")
            else:
                print("\nâš ï¸  ç¿»è¯‘æœªç”Ÿæˆè¾“å‡ºæ–‡ä»¶\n")
        except Exception as e:
            print(f"\nâŒ ç¿»è¯‘å¤±è´¥: {e}\n")

        # 6. ç”ŸæˆAIæ€»ç»“
        print(f"\n{'='*60}")
        print("ğŸ¤– å¼€å§‹ç”ŸæˆAIæ€»ç»“")
        print(f"{'='*60}\n")

        try:
            from ai_summarizer import AISummarizer

            # ç”Ÿæˆå›½å†…æ–°é—»æ€»ç»“
            print("ğŸ“ ç”Ÿæˆå›½å†…æ–°é—»AIæ€»ç»“...")
            combined_file = results.get('combined', {}).get('file')
            if combined_file and os.path.exists(combined_file):
                domestic_summary = AISummarizer.run_from_file(
                    combined_file,
                    'domestic',
                    'ai_summary_domestic.json'
                )
                if domestic_summary.get('success'):
                    print(f"âœ… å›½å†…æ–°é—»AIæ€»ç»“ç”ŸæˆæˆåŠŸ\n")
                else:
                    print(f"âš ï¸  å›½å†…æ–°é—»AIæ€»ç»“ç”Ÿæˆå¤±è´¥: {domestic_summary.get('error')}\n")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°å›½å†…æ–°é—»æ•°æ®æ–‡ä»¶ï¼Œè·³è¿‡å›½å†…æ–°é—»æ€»ç»“\n")

            # ç”Ÿæˆå›½é™…æ–°é—»æ€»ç»“
            print("ğŸ“ ç”Ÿæˆå›½é™…æ–°é—»AIæ€»ç»“...")
            if translator_output and os.path.exists(translator_output):
                international_summary = AISummarizer.run_from_file(
                    translator_output,
                    'international',
                    'ai_summary_international.json'
                )
                if international_summary.get('success'):
                    print(f"âœ… å›½é™…æ–°é—»AIæ€»ç»“ç”ŸæˆæˆåŠŸ\n")
                else:
                    print(f"âš ï¸  å›½é™…æ–°é—»AIæ€»ç»“ç”Ÿæˆå¤±è´¥: {international_summary.get('error')}\n")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°ç¿»è¯‘æ–‡ä»¶ï¼Œè·³è¿‡å›½é™…æ–°é—»æ€»ç»“\n")

            print(f"{'='*60}")
            print("ğŸ‰ AIæ€»ç»“ä»»åŠ¡å®Œæˆ")
            print(f"{'='*60}\n")

        except ImportError:
            print("âš ï¸  AIæ€»ç»“æ¨¡å—æœªå®‰è£…æˆ–ç¯å¢ƒå˜é‡æœªé…ç½®ï¼Œè·³è¿‡AIæ€»ç»“\n")
        except Exception as e:
            print(f"âŒ AIæ€»ç»“ç”Ÿæˆå¤±è´¥: {e}\n")

        return {
            "results": results,
            "total_success": total_success,
            "total_count": total_count,
            "failed_crawlers": failed_crawlers
        }

    except Exception as e:
        print(f"\nâŒ çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}\n")
        return {
            "results": {},
            "total_success": 0,
            "total_count": 0,
            "failed_crawlers": crawlers
        }


def setup_scheduler():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡ - æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ç”¨äºæµ‹è¯•"""
    # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼ˆæµ‹è¯•ç”¨ï¼‰
    schedule.every().hour.do(run_all_crawlers)

    print("å®šæ—¶ä»»åŠ¡å·²è®¾ç½®ï¼Œæ¯å°æ—¶è‡ªåŠ¨è¿è¡Œï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰")
    print("ç¨‹åºæŒç»­è¿è¡Œä¸­...")


def run_scheduler():
    """è¿è¡Œè°ƒåº¦å™¨"""
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡


if __name__ == "__main__":
    import sys

    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == "now":
            # ç«‹å³è¿è¡Œä¸€æ¬¡
            run_all_crawlers(timeout_per_crawler=600)
        elif sys.argv[1] == "daily":
            # è®¾ç½®ä¸ºæ¯æ—¥è¿è¡Œæ¨¡å¼
            schedule.clear()
            schedule.every().day.at("09:00").do(run_all_crawlers)
            print("å·²è®¾ç½®ä¸ºæ¯æ—¥ä¸Šåˆ9ç‚¹è¿è¡Œæ¨¡å¼")
            run_scheduler()
        else:
            print("Usage:")
            print("  python master_crawler.py now      # ç«‹å³è¿è¡Œä¸€æ¬¡")
            print("  python master_crawler.py daily    # è®¾ç½®ä¸ºæ¯æ—¥è¿è¡Œ")
            print("  python master_crawler.py          # é»˜è®¤æ¯å°æ—¶è¿è¡Œï¼ˆæµ‹è¯•ï¼‰")
    else:
        # é»˜è®¤ï¼šè®¾ç½®å®šæ—¶ä»»åŠ¡ï¼ˆæ¯å°æ—¶è¿è¡Œï¼Œæµ‹è¯•ç”¨ï¼‰
        setup_scheduler()
        run_scheduler()
